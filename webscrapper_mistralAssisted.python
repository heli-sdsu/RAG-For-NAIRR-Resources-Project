import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from urllib.robotparser import RobotFileParser
import time
import argparse

def is_valid_url(url, base_url, max_depth, robots_parser):
    parsed_url = urlparse(url)
    path_parts = parsed_url.path.strip('/').split('/')
    if len(path_parts) > max_depth:
        return False
    if not robots_parser.can_fetch('*', url):
        return False
    return url.startswith(base_url)

def extract_text_with_keywords(soup, keywords):
    extracted_texts = []
    for tag in soup.find_all(text=True):
        if any(keyword in tag for keyword in keywords):
            extracted_texts.append(tag.strip())
    return extracted_texts

def crawl(url, base_url, max_depth, visited=None, processed=None, robots_parser=None, keywords=None, output_file=None):
    if visited is None:
        visited = set()
    if processed is None:
        processed = set()

    if url in visited:
        return

    visited.add(url)

    try:
        response = requests.get(url)
        response.raise_for_status()
    except requests.RequestException as e:
        print(f"Failed to retrieve {url}: {e}")
        return

    soup = BeautifulSoup(response.content, 'html.parser')
    if url not in processed:
        extracted_texts = extract_text_with_keywords(soup, keywords)
        if extracted_texts:
            with open(output_file, 'a', encoding='utf-8') as file:
                for text in extracted_texts:
                    file.write(f"URL: {url}\nText: {text}\n\n")
        processed.add(url)

    if max_depth > 0:
        for link in soup.find_all('a', href=True):
            href = link['href']
            full_url = urljoin(url, href)
            if is_valid_url(full_url, base_url, max_depth - 1, robots_parser):
                print(full_url)
                crawl(full_url, base_url, max_depth - 1, visited, processed, robots_parser, keywords, output_file)
                # Be polite and add a delay between requests
                time.sleep(1)
#This needs to be edited out when merging with the RAG since we don't actually need the user input...we'll need to run this likely once per month for each webpage in the JSON NAIRR documentation
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Crawl a website and extract text related to GPUs or CPUs.")
    parser.add_argument("base_url", help="The base URL of the website to crawl.")
    parser.add_argument("max_depth", type=int, help="The maximum depth to crawl.")
    parser.add_argument("--output_file", default="extracted_texts.txt", help="The output file to save extracted text.")

    args = parser.parse_args()

    base_url = args.base_url
    max_depth = args.max_depth
    output_file = args.output_file
    keywords = ["GPU", "CPU"]

    # Parse the robots.txt file
    robots_url = urljoin(base_url, '/robots.txt')
    robots_parser = RobotFileParser()
    robots_parser.set_url(robots_url)
    robots_parser.read()

    # Clear the output file before starting
    open(output_file, 'w').close()

    crawl(base_url, base_url, max_depth, robots_parser=robots_parser, keywords=keywords, output_file=output_file)

